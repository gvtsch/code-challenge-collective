{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cart Pole RL\n",
    "Policy erlenen, indem 100 Epochen durchgeführt und die besten 30 gefiltert und zum Erkennen von Mustern genutzt werden. Werden der 100 Epochen, wird die Policy nicht angepasst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Any, List, Tuple\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Activation, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env: gym.Env) -> None:\n",
    "        self.env = env\n",
    "        self.observations: int = self.env.observation_space.shape[0] # Anzahl Observation States\n",
    "        self.actions: int = self.env.action_space.n # Anzahl Actions (Hier 2)\n",
    "        self.model = self.get_model()\n",
    "\n",
    "    def get_model(self) -> Sequential:\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units=100, input_dim=self.observations))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Dense(units=self.actions))\n",
    "        model.add(Activation(\"softmax\"))\n",
    "        model.summary()\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.007),\n",
    "            loss=\"categorical_crossentropy\",\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def get_action(self, state: Any) -> Any:\n",
    "        state = state.reshape(1, -1)\n",
    "        policy = self.model(state, training=False).numpy()[0] \n",
    "            # [0.7, 0.3] Wahrschlichkeit für jede Aktion in den Step. In Summe immer =1\n",
    "            # Aktion Links tritt mit 70%iger Wahrscheinlichkeit ein\n",
    "            # Kann hier als Policy angesehen werden\n",
    "        return np.random.choice(self.actions, p=policy)\n",
    "\n",
    "    def get_samples(self, num_episodes: int) -> Tuple[List[float], List[Any]]: \n",
    "        rewards = [0.0 for _ in range(num_episodes)]\n",
    "        episodes: List[Any] = [[] for _ in range(num_episodes)]\n",
    "\n",
    "        for episode in range(num_episodes):\n",
    "            state = self.env.reset()\n",
    "            total_reward = 0.0\n",
    "\n",
    "            while True:\n",
    "                action = self.get_action(state)\n",
    "                new_state, reward, done, _ = self.env.step(action)\n",
    "                total_reward += reward\n",
    "                episodes[episode].append((state, action))\n",
    "                state = new_state\n",
    "                if done:\n",
    "                    rewards[episode] = total_reward\n",
    "                    break\n",
    "        return rewards, episodes\n",
    "\n",
    "    def filter_episodes(\n",
    "        self,\n",
    "        rewards: List[float],\n",
    "        episodes: List[Tuple[Any, Any]],\n",
    "        percentile: float,\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, float]:\n",
    "        reward_bound = np.percentile(rewards, percentile)\n",
    "        x_train = []\n",
    "        y_train = []\n",
    "        for reward, epsiode in zip(rewards, episodes):\n",
    "            if reward >= reward_bound:\n",
    "                observation = [step[0] for step in epsiode]\n",
    "                action = [step[1] for step in epsiode]\n",
    "                x_train.extend(observation)\n",
    "                y_train.extend(action)\n",
    "        x_train = np.asarray(x_train)\n",
    "        y_train = to_categorical(y_train, num_classes=self.actions)\n",
    "        return x_train, y_train, reward_bound\n",
    "\n",
    "\n",
    "    def train(\n",
    "        self, percentile: float, num_iterations: int, num_episodes: int\n",
    "    ) -> Tuple[List[float], List[float]]:\n",
    "        reward_means: List[float] = []\n",
    "        reward_bounds: List[float] = []\n",
    "        for it in range(num_iterations):\n",
    "            rewards, episodes = self.get_samples(num_episodes)\n",
    "            x_train, y_train, reward_bound = self.filter_episodes(\n",
    "                rewards, episodes, percentile\n",
    "            )\n",
    "            self.model.train_on_batch(x=x_train, y=y_train)\n",
    "            reward_mean = np.mean(rewards)\n",
    "            print(\n",
    "                f\"Iteration: {it:2d} \"\n",
    "                f\"Reward Mean: {reward_mean:.4f} \"\n",
    "                f\"Reward Bound: {reward_bound:.4f}\"\n",
    "            )\n",
    "            reward_bounds.append(reward_bound)\n",
    "            reward_means.append(reward_mean)\n",
    "        return reward_means, reward_bounds\n",
    "\n",
    "\n",
    "    def play(self, episodes: int, render: bool = True) -> None:\n",
    "        for episode in range(episodes):\n",
    "            state = self.env.reset()\n",
    "            total_reward = 0.0\n",
    "\n",
    "            while True:\n",
    "                if render:\n",
    "                    self.env.render()\n",
    "                action = self.get_action(state)\n",
    "                state, reward, done, _ = self.env.step(action)\n",
    "                total_reward += reward\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            print(f\"Episode: {episode} Total Reward: {total_reward}\")\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main() -> None:\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    agent = Agent(env)\n",
    "    reward_means, reward_bounds = agent.train(percentile=70.0, num_iterations=30, num_episodes=100)\n",
    "    input()\n",
    "        # 30 Episoden mit je 100 Iterationen, die 70% der guten filtern\n",
    "    agent.play(episodes=10)\n",
    "\n",
    "    plt.title(\"Training Performance\")\n",
    "    plt.plot(range(len(reward_means)), reward_means, color=\"red\")\n",
    "    plt.plot(range(len(reward_bounds)), reward_bounds, color=\"blue\")\n",
    "    plt.legend([\"reward_means\", \"reward_bounds\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 100)               500       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 202       \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 702\n",
      "Trainable params: 702\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Iteration:  0 Reward Mean: 21.6600 Reward Bound: 23.0000\n",
      "Iteration:  1 Reward Mean: 23.9900 Reward Bound: 25.0000\n",
      "Iteration:  2 Reward Mean: 26.7300 Reward Bound: 28.3000\n",
      "Iteration:  3 Reward Mean: 31.2600 Reward Bound: 35.3000\n",
      "Iteration:  4 Reward Mean: 36.7600 Reward Bound: 42.3000\n",
      "Iteration:  5 Reward Mean: 42.3500 Reward Bound: 50.0000\n",
      "Iteration:  6 Reward Mean: 41.5800 Reward Bound: 47.3000\n",
      "Iteration:  7 Reward Mean: 51.3500 Reward Bound: 61.0000\n",
      "Iteration:  8 Reward Mean: 49.2800 Reward Bound: 57.0000\n",
      "Iteration:  9 Reward Mean: 53.2700 Reward Bound: 60.0000\n",
      "Iteration: 10 Reward Mean: 51.0600 Reward Bound: 59.0000\n",
      "Iteration: 11 Reward Mean: 57.1500 Reward Bound: 66.0000\n",
      "Iteration: 12 Reward Mean: 61.4600 Reward Bound: 66.3000\n",
      "Iteration: 13 Reward Mean: 66.4300 Reward Bound: 74.3000\n",
      "Iteration: 14 Reward Mean: 69.8800 Reward Bound: 85.0000\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad6c57515631d9bb109eb499b4bb1ec6f506277a5ee28599dfb5ae2805d1ee5a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf_udemy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
